# ML

## References: 
![picture alt](http://scikit-learn.org/stable/_static/ml_map.png)

----

### Day 1: 
#### KNN
* Finding the distance between the test point to all train points, and choose the majority from K nearest neighbour.
* Distance fucntions explained (Euclidean, Manhattan, Minkowski and Hamming): https://www.saedsayad.com/k_nearest_neighbors.htm
* [Source code:] https://www.kaggle.com/vamshavardhanreddy/knn-implementation-using-pandas
	
### Day 2: 
#### Linear regression
* A linear classifier does classification decision based on the value of a linear combination of the characteristics.
* reference: https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/linear_classification.html
* Source code: https://www.kaggle.com/vamshavardhanreddy/linear-regression

#### Feature Selection
* We mainly use feature selection techinques to get insights about the features and their relative importance with the target variable.The idea is to keep most relevant but not redundant feature for predictive model that can yield optimal accuracy.
* source code: https://www.kaggle.com/vamshavardhanreddy/feature-selection	

### Day 3: 
#### Decision Trees
* concept explanation: https://github.com/vamsha/100daysofML/blob/master/Decision_tree
* Source code: https://www.kaggle.com/vamshavardhanreddy/decision-tree-implementation-using-pandas

### Day 4:
* Participated in https://www.kaggle.com/c/titanic kaggle competition.
* preprocessing the data is the primary task.
* Applied Decision tree and Random forest algorithms.

### Day 5:
#### Random Forest
* Concept explanation : https://github.com/vamsha/100daysofML/blob/master/Decision_tree
* Source Code: https://www.kaggle.com/vamshavardhanreddy/randomforest

#### Ensemble Learning
* Concept explanation : https://github.com/vamsha/100daysofML/blob/master/Ensemble%20Learning
* Source Code: https://www.kaggle.com/vamshavardhanreddy/ensemble-learning

### Day 6:
#### Gradient Descent
* Concept explanation: https://github.com/vamsha/100daysofML/blob/master/Gradient%20Descent
* Maths behind GD: https://www.youtube.com/watch?v=jc2IthslyzM&list=LLLyTovQpha5RMvWHI8JOB7g&index=12&t=2s (worth watching)
* Source code:
	https://www.kaggle.com/vamshavardhanreddy/mini-batch-gradient-descent
	https://www.kaggle.com/vamshavardhanreddy/stochastic-gradient-descent
	https://www.kaggle.com/vamshavardhanreddy/batch-gradient-descent
	https://www.kaggle.com/vamshavardhanreddy/gradient-descent-local-minima

### Day 7:
#### Performance Evaluation
* Concept explanation:https://github.com/vamsha/100daysofML/blob/master/Performance%20Evaluation
* Source code: https://www.kaggle.com/vamshavardhanreddy/performance-metrics

### Day 8:
#### PCA
* Concept explanation:https://github.com/vamsha/100daysofML/blob/master/PCA
* Source code: https://www.kaggle.com/vamshavardhanreddy/pca-principal-component-analysis


